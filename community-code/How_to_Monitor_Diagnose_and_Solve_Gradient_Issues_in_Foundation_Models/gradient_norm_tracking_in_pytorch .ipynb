{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31e8adca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/klea.ziu/.conda/envs/OCM/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import DataCollatorWithPadding\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from random import random\n",
    "from neptune_scale import Run\n",
    "from transformers import AdamW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7aa46cb",
   "metadata": {},
   "source": [
    "# Step 2. Load and Preprocess Dataset\n",
    "We will use the GLUE dataset and transformers from HuggingFace. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29dd0293",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/klea.ziu/.conda/envs/OCM/lib/python3.9/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset('glue', name='mrpc')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['sentence1'], examples['sentence2'], \n",
    "                     truncation=True, padding=\"longest\", return_tensors=\"pt\")\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_datasets.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"token_type_ids\", \"label\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0a21ba",
   "metadata": {},
   "source": [
    "# Step 3. Prepare the Dataloaders and load the Bert model for sequence classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3275798a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = tokenized_datasets['train'].shuffle(seed=42).select(range(1000))  # Sample for demonstration\n",
    "eval_dataset = tokenized_datasets['validation'].shuffle(seed=42).select(range(408))\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=8, collate_fn=data_collator)\n",
    "eval_dataloader = DataLoader(eval_dataset, batch_size=8, collate_fn=data_collator)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_datasets.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"token_type_ids\", \"label\"])\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "# Move model to CUDA\n",
    "model.to('cuda')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db51706",
   "metadata": {},
   "source": [
    "# Step 4. Initialize Neptune Scale for Logging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1533553a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import random\n",
    "from neptune_scale import Run\n",
    "\n",
    "custom_id = random()\n",
    "\n",
    "run = Run(\n",
    "    api_token=\"YOUR_API_TOKEN\",# replace with your Neptune API token\n",
    "    project=\"your_workspace/your_project\", # replace with your workspace and project name\n",
    "    experiment_name=\"gradient_tracking\",\n",
    "    run_id=f\"gradient-{custom_id}\",\n",
    ")\n",
    "\n",
    "run.log_configs({\n",
    "    \"learning_rate\": 5e-5,\n",
    "    \"batch_size\": 8,\n",
    "    \"optimizer\": \"AdamW\",\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057ad307",
   "metadata": {},
   "source": [
    "# Step 5. Define the Gradient Norm Logging Function\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3e54be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_gradient_norms(model, step):\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.grad is not None:\n",
    "            grad_norm = param.grad.norm().item()\n",
    "            run.log_metrics({\"gradients/\" + name: grad_norm}, step=step)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6522173a",
   "metadata": {},
   "source": [
    "# Step 6. Train the Model and Track Gradients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f912c7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(10):\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        inputs = {k: v.to('cuda') for k, v in batch.items() if k in tokenizer.model_input_names}\n",
    "        labels = batch['labels'].to('cuda')\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(**inputs, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        # Log gradient norms\n",
    "        log_gradient_norms(model, step + epoch * len(train_dataloader))\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        # Log Loss to Neptune Scale\n",
    "        run.log_metrics({\"loss\": loss.item()}, step=step + epoch * len(train_dataloader))\n",
    "\n",
    "# Add tags and close the run\n",
    "run.add_tags([\"gradient_tracking\", \"pytorch\", \"transformers\"])\n",
    "run.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "OCM",
   "language": "python",
   "name": "ocm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
